{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPE Tokenizer\n",
    "\n",
    "> Tokenize SMILES (Simplified Molecular-Input Line-Entry System) into substructure units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import codecs\n",
    "import io\n",
    "import argparse\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "def learn_SPE(infile, outfile, num_symbols, min_frequency=2, augmentation=0, verbose=False, total_symbols=False):\n",
    "    \"\"\"\n",
    "    Learn num_symbols SPE operations from infile and write to outfile.\n",
    "    \n",
    "    *infile*: a list of SMILES\n",
    "    \n",
    "    *num_symbols*: maximum total number of SPE symbols \n",
    "    \n",
    "    *min_frequency*: the minimum frequency of SPE symbols appears.\n",
    "    \n",
    "    *augmentation*: times of SMILES augmentation\n",
    "    \n",
    "    *verbose*: if True, print the merging process\n",
    "    \n",
    "    *total_symbols*: if True; the maximum total of SPE symbols = num_symbols - number of atom-level tokens\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = get_vocabulary(infile, augmentation=augmentation)\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    stats, indices = get_pair_statistics(sorted_vocab)\n",
    "    big_stats = copy.deepcopy(stats)\n",
    "\n",
    "    if total_symbols:\n",
    "        uniq_char = set()\n",
    "        for word in vocab:\n",
    "            for char in word:\n",
    "                uniq_char.add(char)\n",
    "        sys.stderr.write(f'Number of unique characters & Reducing number of merge operations by: {len(uniq_char)}\\n')\n",
    "        sys.stderr.write(f'Unique characters: {(uniq_char)}\\n')\n",
    "        num_symbols -= len(uniq_char)\n",
    "        for i in uniq_char:\n",
    "            vocab_index2units2freq[i] = 0\n",
    "                    \n",
    "    # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "    threshold = max(stats.values()) / 10\n",
    "    for i in range(num_symbols):\n",
    "        if stats:\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "\n",
    "        # we probably missed the best pair because of pruning; go back to full statistics\n",
    "        if not stats or (i and stats[most_frequent] < threshold):\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "            stats = copy.deepcopy(big_stats)\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "            # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "            threshold = stats[most_frequent] * i/(i+10000.0)\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "        if stats[most_frequent] < min_frequency:\n",
    "            sys.stderr.write('no pair has frequency >= {0}. Stopping\\n'.format(min_frequency))\n",
    "            break\n",
    "\n",
    "        s1 = most_frequent[0]\n",
    "        s2 = most_frequent[1]\n",
    "        \n",
    "        vocab_index2units2freq[s1+s2] = stats[most_frequent]\n",
    "        \n",
    "        if verbose:\n",
    "            sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n",
    "        outfile.write('{0} {1}\\n'.format(*most_frequent))\n",
    "        freq_codes.append(most_frequent)\n",
    "        changes = replace_pair(most_frequent, sorted_vocab, indices)\n",
    "        update_pair_statistics(most_frequent, changes, stats, indices)\n",
    "        stats[most_frequent] = 0\n",
    "        if not i % 100:\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "class SPE_Tokenizer(object):\n",
    "    \"\"\"\n",
    "    Tokenize SMILES based on the learned SPE tokens.\n",
    "    \n",
    "    codes: output file of `learn_SPE()`\n",
    "    \n",
    "    merges: number of learned SPE tokens you want to use. `-1` means using all of them. `1000` means use the most frequent 1000.\n",
    "    \n",
    "    exclusive_tokens: argument that passes to  `atomwise_tokenizer()`\n",
    "    \n",
    "    glossaries: argument that passes to `isolate_glossary()`\n",
    "    \n",
    "    dropout: See [BPE-Dropout: Simple and Effective Subword Regularization](https://arxiv.org/abs/1910.13267).\n",
    "    If `dropout` is set to 0, the segmentation is equivalent to the standard BPE; if `dropout` is set to 1, the segmentation splits words into distinct characters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, codes, merges=-1, glossaries=None, exclusive_tokens=None):\n",
    "\n",
    "        codes.seek(0)\n",
    "        offset=1\n",
    "\n",
    "        self.bpe_codes = [tuple(item.strip('\\r\\n ').split(' ')) for (n, item) in enumerate(codes) if (n < merges or merges == -1)]\n",
    "\n",
    "        for i, item in enumerate(self.bpe_codes):\n",
    "            if len(item) != 2:\n",
    "                sys.stderr.write('Error: invalid line {0} in BPE codes file: {1}\\n'.format(i+offset, ' '.join(item)))\n",
    "                sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\\n')\n",
    "                sys.exit(1)\n",
    "\n",
    "        # some hacking to deal with duplicates (only consider first instance)\n",
    "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
    "        \n",
    "        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n",
    "        \n",
    "        self.glossaries = glossaries if glossaries else []\n",
    "\n",
    "        self.glossaries_regex = re.compile('^({})$'.format('|'.join(glossaries))) if glossaries else None\n",
    "        \n",
    "        self.exclusive_tokens = exclusive_tokens\n",
    "        self.cache = {}\n",
    "    \n",
    "    def tokenize(self, smi, dropout=0):\n",
    "        segments = [out for segment in self._isolate_glossaries(smi)\n",
    "                    for out in encode(segment,\n",
    "                                      self.bpe_codes,\n",
    "                                      self.bpe_codes_reverse,\n",
    "                                      self.cache,\n",
    "                                      self.exclusive_tokens,\n",
    "                                      self.glossaries_regex,\n",
    "                                      dropout)]\n",
    "        return ' '.join(segments)\n",
    "        \n",
    "\n",
    "    def _isolate_glossaries(self, word):\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                                 for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments\n",
    "\n",
    "\n",
    "def encode(orig, bpe_codes, bpe_codes_reverse, cache, \n",
    "           exclusive_tokens=None, glossaries_regex=None, dropout=0):\n",
    "    \"\"\"Encode word based on list of SPE merge operations, which are applied consecutively.\n",
    "    \"\"\"\n",
    "\n",
    "    if not dropout and orig in cache:\n",
    "        return cache[orig]\n",
    "\n",
    "    if glossaries_regex and glossaries_regex.match(orig):\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    if len(orig) == 1:\n",
    "        return orig\n",
    "    \n",
    "    word = atomwise_tokenizer(orig, exclusive_tokens=exclusive_tokens)\n",
    "\n",
    "    while len(word) > 1:\n",
    "\n",
    "        # get list of symbol pairs; optionally apply dropout\n",
    "        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        #get first merge operation in list of BPE codes\n",
    "        bigram = min(pairs)[2]\n",
    "\n",
    "        # find start position of all pairs that we want to merge\n",
    "        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n",
    "\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        bigram = ''.join(bigram)\n",
    "        for j in positions:\n",
    "            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n",
    "            if j < i:\n",
    "                continue\n",
    "            new_word.extend(word[i:j]) # all symbols before merged pair\n",
    "            new_word.append(bigram) # merged pair\n",
    "            i = j+2 # continue after merged pair\n",
    "        new_word.extend(word[i:]) # add all symbols until end of word\n",
    "        word = new_word\n",
    "\n",
    "    word = tuple(word)\n",
    "\n",
    "    cache[orig] = word\n",
    "    return word\n",
    "\n",
    "def isolate_glossary(word, glossary):\n",
    "    \"\"\"\n",
    "    Isolate a glossary present inside a word.\n",
    "    \n",
    "    Returns a list of subwords. In which all 'glossary' glossaries are isolated.\n",
    "    \n",
    "    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n",
    "        ['1934', 'USA', 'B', 'USA']\n",
    "    \"\"\"\n",
    "    # regex equivalent of (if word == glossary or glossary not in word)\n",
    "    if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n",
    "        return [word]\n",
    "    else:\n",
    "        segments = re.split(r'({})'.format(glossary), word)\n",
    "        segments, ending = segments[:-1], segments[-1]\n",
    "        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n",
    "        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
